\begin{frame}{data challenge revisited}{insufficiency vs. representativeness}
		\hspace{-4mm}moderate improvements can be made to deal with insufficient data, but
		\visible<2->{
		\bigskip
		\question{is the amount of data really the main issue}}
		
		\begin{itemize}
				\item	maybe not...
				\begin{itemize}
					\item a closer look at example music datasets for popular tasks
				\end{itemize}
		\end{itemize}
\end{frame}

\begin{frame}{data challenge revisited}{dataset example 1: chord detection}
				\begin{itemize}
						\item	Beatles dataset for chord detection
								\begin{itemize}
										\item contains all Beatles albums
										\item	chord vocabulary
										\item	problem stylistic homogeneity (timbre, harmony progressions, release times, ...)
										\item problem imbalance (get numbers), leads to only the top classes being classified, potential key dependence
								\end{itemize}
				\end{itemize}
				%\item	source separation: MUSDB
					%\begin{itemize}
						%\item choice of classes
					%\end{itemize}
				%\end{itemize}
				%\item homogeneity questions representativeness of data
				%\item	category restrictions question generalization to task
				%\item problem for methodology: 
				%\item	problem for training: 
				%\item	problem for testing: 
\end{frame}

\begin{frame}{data challenge revisited}{dataset example 2: music genre classification}
		\begin{itemize}
			\item	GTZAN dataset for genre classification
					\begin{itemize}
							\item 10 classes - what are the most important music genres
							\item	problem labels don't match 'real' task (categories and single label classification)
					\end{itemize}
		\end{itemize}
\end{frame}

\begin{frame}{data challenge revisited}{dataset example 3: source separation}
		\begin{itemize}
			\item	MUSDB dataset for source separation
					\begin{itemize}
							\item 4 tracks - what tracks are most important
							\item	problem tracks do not reflect real needs for source separation
					\end{itemize}
		\end{itemize}
\end{frame}

\begin{frame}{data challenge revisited}{dataset examples: summary}
		\begin{itemize}
			\item 	false homogeneity/\textbf{non-representativeness impacts generalization}
				\begin{itemize}
					\item system cannot learn what is hasn't seen or what seems irrelevant
				\end{itemize}
			\bigskip
			\item		imbalance can lead to \textbf{unwanted bias} 
				\begin{itemize}
					\item 	\textit{training}: system wrongly favors certain categories
					\item		\textit{testing}: results may imply good performance yet cannot be generalized
				\end{itemize}
			\bigskip
			\item		mismatch between dataset labels and real task may \textbf{feign good performance}
				\begin{itemize}
					\item 	misleading results
					\item		architectural bias
				\end{itemize}
		\end{itemize}
		%\begin{itemize}
			%\item 	systems might not generalize
			%\item		there is no way to judge as the evaluation is done on data with similar characteristics
				%\begin{itemize}
					%\item results may imply good performance yet cannot be generalized
				%\end{itemize}
			%\item		annotation choices and limitations might impact architectural decisions
		%\end{itemize}
\end{frame}