
@book{lerch_introduction_2012,
	address = {Hoboken},
	title = {An {Introduction} to {Audio} {Content} {Analysis}: {Applications} in {Signal} {Processing} and {Music} {Informatics}},
	isbn = {978-1-118-26682-3},
	abstract = {With the proliferation of digital audio distribution over digital media, audio content analysis is fast becoming a requirement for designers of intelligent signal-adaptive audio processing systems. Written by a well-known expert in the field, this book provides quick access to different analysis algorithms and allows comparison between different approaches to the same task, making it useful for newcomers to audio signal processing and industry experts alike. A review of relevant fundamentals in audio signal processing, psychoacoustics, and music theory, as well as downloadable MATLAB files are also included. Please visit the companion website: www.AudioContentAnalysis.org},
	publisher = {Wiley-IEEE Press},
	author = {Lerch, Alexander},
	year = {2012},
	keywords = {analysis, audio, audio signal processing, information, listening, machine, machine listening, music, music analysis, music information retrieval, processing, retrieval, signal},
	file = {Lerch_2012_An Introduction to Audio Content Analysis.pdf:H\:\\Docs\\zotero\\storage\\X9NPB4I9\\Lerch_2012_An Introduction to Audio Content Analysis.pdf:application/pdf}
}

@inproceedings{burred_hierarchical_2003,
	address = {London},
	title = {A {Hierarchical} {Approach} to {Automatic} {Musical} {Genre} {Classification}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Burred-and-Lerch-2003-A-Hierarchical-Approach-to-Automatic-Musical-Genre.pdf},
	doi = {10.1.1.2.6582},
	abstract = {A system for the automatic classification of audio signals according to audio category is presented. The signals are recognized as speech, background noise and one of 13 musical genres. A large number of audio features are evaluated for their suitability in such a classification task, including well-known physical and perceptual features, audio descriptors defined in the MPEG-7 standard, as well as new features proposed in this work. These are selected with regard to their ability to distinguish between a given set of audio types and to their robustness to noise and bandwidth changes. In contrast to previous systems, the feature selection and the classification process itself are carried out in a hierarchical way. This is motivated by the numerous advantages of such a tree-like structure, which include easy expansion capabilities, flexibility in the design of genre-dependent features and the ability to reduce the probability of costly errors. The resulting application is evaluated with respect to classification accuracy and computational costs.},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Digital} {Audio} {Effects} ({DAFX})},
	author = {Burred, Juan José and Lerch, Alexander},
	month = sep,
	year = {2003},
	file = {Burred and Lerch - 2003 - A Hierarchical Approach to Automatic Musical Genre.pdf:H\:\\Docs\\zotero\\storage\\EXU2F4V2\\Burred and Lerch - 2003 - A Hierarchical Approach to Automatic Musical Genre.pdf:application/pdf}
}

@inproceedings{kraft_tonalness_2013,
	address = {Maynooth},
	title = {The {Tonalness} {Spectrum}: {Feature}-{Based} {Estimation} of {Tonal} {Components}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/04/Kraft%20et%20al_2013_The%20Tonalness%20Spectrum.pdf},
	abstract = {The tonalness spectrum shows the likelihood of a spectral bin be-
ing part of a tonal or non-tonal component. It is a non-binary
measure based on a set of established spectral features. An eas-
ily extensible framework for the computation, selection, and com-
bination of features is introduced. The results are evaluated and
compared in two ways. First with a data set of synthetically gen-
erated signals but also with real music signals in the context of a
typical MIR application.},
	urldate = {2014-01-16},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Digital} {Audio} {Effects}},
	author = {Kraft, Sebastian and Lerch, Alexander and Zölzer, Udo},
	year = {2013},
	file = {Kraft et al. - 2013 - The Tonalness Spectrum Feature-Based Estimation o.pdf:H\:\\Docs\\zotero\\storage\\IDZVSME7\\Kraft et al. - 2013 - The Tonalness Spectrum Feature-Based Estimation o.pdf:application/pdf}
}

@inproceedings{ness_strategies_2011,
	address = {Hangzhou},
	title = {Strategies for {Orca} {Call} {Retrieval} to {Support} {Collaborative} {Annotation} of a {Large} {Archive}},
	isbn = {978-1-4577-1434-4},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6093798},
	doi = {10.1109/MMSP.2011.6093798},
	abstract = {The Orchive is a large audio archive of hydrophone recordings of Killer whale (Orcinus orca) vocalizations. Researchers and users from around the world can interact with the archive using a collaborative web-based annotation, visualization and retrieval interface. In addition a mobile client has been written in order to crowdsource Orca call annotation. In this paper we describe and compare different strategies for the retrieval of discrete Orca calls. In addition, the results of the automatic analysis are integrated in the user interface facilitating annotation as well as leveraging the existing annotations for supervised learning. The best strategy achieves a mean average precision of 0.77 with the first retrieved item being relevant 95\% of the time in a dataset of 185 calls belonging to 4 types.},
	booktitle = {Proceedings of the {International} {Workshop} on {Multimedia} {Signal} {Processing} ({MMSP})},
	publisher = {IEEE},
	author = {Ness, Steven R and Lerch, Alexander and Tzanetakis, George},
	year = {2011},
	file = {Ness et al. - 2011 - Strategies for Orca Call Retrieval to Support Coll.pdf:H\:\\Docs\\zotero\\storage\\HBV47IGU\\Ness et al. - 2011 - Strategies for Orca Call Retrieval to Support Coll.pdf:application/pdf}
}

@inproceedings{coler_cmmsd:_2014,
	address = {London, UK},
	title = {{CMMSD}: {A} {Data} {Set} for {Note}-{Level} {Segmentation} of {Monophonic} {Music}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/04/Coler_Lerch_2014_CMMSD.pdf},
	abstract = {A musical data set for note-level segmentation of monophonic music is presented. It contains 36 excerpts from
commercial recordings of monophonic classical western music and features the instrument groups strings,
woodwind and brass. The excerpts are self-contained phrases with a mean length of 17.97 seconds and an
average of 20 notes. All phrases are played in moderate tempo, mostly with significant amounts of expressive
articulation. A manually annotated ground truth splits each item into a sequence of the three states note,
transition and rest. The set is designed as an open source project, aiming at the development and evaluation
of algorithms for segmentation, music performance analysis and feature selection. This paper presents the
process of ground truth labeling and a detailed description of the data set and its properties.},
	booktitle = {Proceedings of the {AES} 53rd {International} {Conference} on {Semantic} {Audio}},
	publisher = {Audio Engineering Society (AES)},
	author = {Coler, Henrik von and Lerch, Alexander},
	year = {2014},
	file = {Coler and Lerch - 2014 - CMMSD A Data Set for Note-Level Segmentation of M.pdf:H\:\\Docs\\zotero\\storage\\2NUMUB6D\\Coler and Lerch - 2014 - CMMSD A Data Set for Note-Level Segmentation of M.pdf:application/pdf}
}

@phdthesis{lerch_qualitatsbeurteilung_2001,
	title = {Qualitätsbeurteilung von codierten {Audiosignalen} mittels eines objektiven {Verfahrens} {Diplomarbeit}},
	school = {Technische Universität Berlin},
	author = {Lerch, Alexander},
	year = {2001},
	file = {Lerch - 2001 - Qualitätsbeurteilung von codierten Audiosignalen m.pdf:H\:\\Docs\\zotero\\storage\\D6CFUHH8\\Lerch - 2001 - Qualitätsbeurteilung von codierten Audiosignalen m.pdf:application/pdf}
}

@article{burred_hierarchical_2004,
	title = {Hierarchical {Automatic} {Audio} {Signal} {Classification}},
	volume = {52},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Burred-and-Lerch-2004-Hierarchical-Automatic-Audio-Signal-Classification.pdf},
	abstract = {The design, implementation, and evaluation of a system for automatic audio signal classification is presented. The signals are classified according to audio type, differentiating between three speech classes, 13 musical genres, and background noise. A large number of audio features are evaluated for their suitability in such a classification task, including MPEG-7 descriptors and several new features. The selection of the features is carried out systematically with regard to their robustness to noise and bandwidth changes, as well as to their ability to distinguish a given set of audio types. Direct and hierarchical approaches for the feature selection and for the classification are evaluated and compared.},
	number = {7/8},
	journal = {Journal of the Audio Engineering Society (JAES)},
	author = {Burred, Juan José and Lerch, Alexander},
	year = {2004},
	pages = {724--739},
	file = {Burred and Lerch - 2004 - Hierarchical Automatic Audio Signal Classification.pdf:H\:\\Docs\\zotero\\storage\\WR34F49E\\Burred and Lerch - 2004 - Hierarchical Automatic Audio Signal Classification.pdf:application/pdf}
}

@book{lerch_software-based_2009,
	address = {München},
	title = {Software-{Based} {Extraction} of {Objective} {Parameters} from {Music} {Performances}},
	isbn = {978-3-640-29496-1},
	url = {http://dx.doi.org/10.14279/depositonce-2025},
	abstract = {Different music performances of the same score may significantly differ from each other. It is obvious that not only the composer’s work, the score, defines the listener’s music experience, but that the music performance itself is an integral part of this experience. Music performers use the information contained in the score, but interpret, transform or add to this information. Four parameter classes can be used to describe a performance objectively: tempo and timing, loudness, timbre and pitch. Each class contains a multitude of individual parameters that are at the performers’ disposal to generate a unique physical rendition of musical ideas. The extraction of such objective parameters is one of the difficulties in music performance research. This work presents an approach to the software-based extraction of tempo and timing, loudness and timbre parameters from audio files to provide a tool for the automatic parameter extraction from music performances. The system is applied to extract data from 21 string quartet performances and a detailed analysis of the extracted data is presented. The main contributions of this thesis are the adaptation and development of signal processing approaches to performance parameter extraction and the presentation and discussion of string quartet performances of a movement of Beethoven’s late String Quartet op. 130.},
	publisher = {GRIN Verlag},
	author = {Lerch, Alexander},
	year = {2009},
	keywords = {analysis, audio, content, information, music, performance, retrieval},
	file = {Lerch - 2009 - Software-Based Extraction of Objective Parameters .pdf:H\:\\Docs\\zotero\\storage\\2GQAJ3PN\\Lerch - 2009 - Software-Based Extraction of Objective Parameters .pdf:application/pdf}
}

@inproceedings{yogev_system_2008,
	address = {Leipzig},
	title = {A {System} for {Automatic} {Audio} {Harmonization} ( {Ein} {System} für automatische {Audio}-{Harmonisierung})},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Yogev-and-Lerch-2008-A-System-for-Automatic-Audio-Harmonization-Ein-S-1.pdf},
	doi = {10.1.1.148.8391},
	abstract = {A rule-based system for automatic melody harmonization is presented. It models the cognitive process a human arranger undergoes when confronted with the same task, namely: segmenting the melody into phrases, tagging melody notes with harmonic functions, establishing a palette of possible chords for each note, and finding the most agreeable voicing through these chords. The system is designed to be embedded in an audio framework, which synthe- sizes a four-voiced audio output using pitch-shifting techniques. Principles of classical counterpoint as well as common voice-leading conven- tions are utilized by the system. We shall outline the various phases of computa- tion, describe the rules applied in each phase, and present perspectives regarding the stylistic flexibility suggested by the system's design.},
	booktitle = {Proceedings of the {VdT} {International} {Convention} (25. {Tonmeistertagung})},
	author = {Yogev, Noam and Lerch, Alexander},
	year = {2008},
	keywords = {audio, harmonization},
	file = {Yogev and Lerch - 2008 - A System for Automatic Audio Harmonization ( Ein S.pdf:H\:\\Docs\\zotero\\storage\\22F2DX2W\\Yogev and Lerch - 2008 - A System for Automatic Audio Harmonization ( Ein S.pdf:application/pdf}
}

@techreport{lerch_evaluation_2005,
	address = {Berlin},
	type = {White {Paper}},
	title = {On the {Evaluation} of {Automatic} {Onset} {Tracking} {Systems}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Lerch-and-Klich-2005-On-the-Evaluation-of-Automatic-Onset-Tracking-Syst.pdf},
	abstract = {This paper summarizes the problems, definitions and requirements that are important for the evaluation of onset tracking systems for audio signals in PCM format. Different procedures and metrics for evaluation and parametrization are presented and commented. Overall, a complete methodology for the evaluation of automatic onset detection systems is proposed.},
	institution = {zplane.development},
	author = {Lerch, Alexander and Klich, Ingmar-Leander},
	year = {2005},
	annote = {available online (12/2005): \{http://www.zplane.de/Downloads/OnTheEvaluationOfAutomaticOnsetTrackingSystems.pdf\}},
	file = {Lerch and Klich - 2005 - On the Evaluation of Automatic Onset Tracking Syst.pdf:H\:\\Docs\\zotero\\storage\\MMGQPUSP\\Lerch and Klich - 2005 - On the Evaluation of Automatic Onset Tracking Syst.pdf:application/pdf}
}

@incollection{lerch_software-gestutzte_2011,
	address = {Mainz},
	series = {Klang und {Begriff}},
	title = {Software-gestützte {Merkmalsextraktion} für die musikalische {Aufführungsanalyse}},
	isbn = {978-3-7957-0771-2},
	booktitle = {Gemessene {Interpretation} - {Computergestützte} {Aufführungsanalyse} im {Kreuzverhör} der {Disziplinen}},
	publisher = {Schott},
	author = {Lerch, Alexander},
	editor = {von Loesch, Heinz and Weinzierl, Stefan},
	year = {2011},
	pages = {205--212},
	file = {Lerch - 2011 - Software-gestützte Merkmalsextraktion für die musi.pdf:H\:\\Docs\\zotero\\storage\\BPBUZE5A\\Lerch - 2011 - Software-gestützte Merkmalsextraktion für die musi.pdf:application/pdf}
}

@incollection{lerch_digitale_2008,
	address = {Berlin},
	title = {Digitale {Audiotechnik}: {Grundlagen}},
	isbn = {978-3-540-34300-4},
	booktitle = {Handbuch der {Audiotechnik}},
	publisher = {Springer},
	author = {Lerch, Alexander and Weinzierl, Stefan},
	editor = {Weinzierl, Stefan},
	year = {2008},
	pages = {785--811},
	file = {Lerch and Weinzierl - 2008 - Digitale Audiotechnik Grundlagen.pdf:H\:\\Docs\\zotero\\storage\\Z5G6VIDT\\Lerch and Weinzierl - 2008 - Digitale Audiotechnik Grundlagen.pdf:application/pdf}
}

@incollection{lerch_bitratenreduktion_2008,
	address = {Berlin},
	title = {Bitratenreduktion},
	isbn = {978-3-540-34300-4},
	booktitle = {Handbuch der {Audiotechnik}},
	publisher = {Springer},
	author = {Lerch, Alexander},
	editor = {Weinzierl, Stefan},
	year = {2008},
	pages = {849--884},
	file = {Lerch - 2008 - Bitratenreduktion.pdf:H\:\\Docs\\zotero\\storage\\WIWTN22B\\Lerch - 2008 - Bitratenreduktion.pdf:application/pdf}
}

@inproceedings{lerch_feapi:_2005,
	address = {Madrid},
	title = {{FEAPI}: {A} {Low} {Level} {Feature} {Extraction} {Plugin} {API}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Lerch-et-al.-2005-FEAPI-A-Low-Level-Feature-Extraction-Plugin-API.pdf},
	abstract = {This paper presents FEAPI, an easy-to-use platform-independent
plugin application programming interface (API) for the extraction
of low level features from audio in PCM format in the context of
music information retrieval software. The need for and advantages
of using an open and well-defined plugin interface are outlined in
this paper and an overview of the API itself and its usage is given.},
	booktitle = {Proceedings of 8th {International} {Conference} on {Digital} {Audio} {Effects} ({DAFX})},
	author = {Lerch, Alexander and Eisenberg, Gunnar and Tanghe, Koen},
	month = sep,
	year = {2005},
	file = {Lerch et al. - 2005 - FEAPI A Low Level Feature Extraction Plugin API.pdf:H\:\\Docs\\zotero\\storage\\3D2X82DC\\Lerch et al. - 2005 - FEAPI A Low Level Feature Extraction Plugin API.pdf:application/pdf}
}

@inproceedings{wiesener_adaptive_2010,
	address = {London},
	title = {Adaptive {Noise} {Reduction} for {Real}-time {Applications}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/04/Wiesener%20et%20al_2010_Adaptive%20Noise%20Reduction%20for%20Real-time%20Applications.pdf},
	abstract = {We present a new algorithm for real-time noise reduction of audio signals. In order to derive the noise reduction function, the proposed method adaptively estimates the instantaneous noise spectrum from an autoregressive signal model as opposed to the widely-used approach of using a constant noise spectrum fingerprint. In conjunction with the Ephraim and Malah suppression rule a significant reduction of both stationary and non-stationary noise can be obtained. The adaptive algorithm is able to work without user interaction and is capable of real-time processing. Furthermore, quality improvements are easily possible by integration of additional processing blocks such as transient preservation.},
	booktitle = {Proceedings of the 128th {Audio} {Engineering} {Society} {Convention} ({Preprint} \#8048)},
	publisher = {Audio Engineering Society},
	author = {Wiesener, Constantin and Flohrer, Tim and Lerch, Alexander and Weinzierl, Stefan},
	month = may,
	year = {2010},
	file = {Wiesener et al. - 2010 - Adaptive Noise Reduction for Real-time Application.pdf:H\:\\Docs\\zotero\\storage\\BVXT8RTG\\Wiesener et al. - 2010 - Adaptive Noise Reduction for Real-time Application.pdf:application/pdf}
}

@inproceedings{lerch_requirement_2006,
	address = {Victoria},
	title = {On the {Requirement} of {Automatic} {Tuning} {Frequency} {Estimation}},
	url = {http://dx.doi.org/10.14279/depositonce-2037},
	abstract = {The deviation of the tuning frequency from the standard tuning frequency 440 Hz is evaluated for a database of classical music. It is discussed if and under what circumstances such a deviation may affect the robustness of pitch-based systems for musical content analysis.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Lerch, Alexander},
	year = {2006},
	keywords = {frequency, tuning},
	file = {Lerch - 2006 - On the Requirement of Automatic Tuning Frequency E.pdf:H\:\\Docs\\zotero\\storage\\6H9TWH6N\\Lerch - 2006 - On the Requirement of Automatic Tuning Frequency E.pdf:application/pdf}
}

@inproceedings{lerch_ansatz_2004,
	address = {Leipzig},
	title = {Ein {Ansatz} zur automatischen {Erkennung} der {Tonart} in {Musikdateien}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Lerch-2004-Ein-Ansatz-zur-automatischen-Erkennung-der-Tonart-.pdf},
	abstract = {Es wird ein Verfahren zur automatischen Erkennung der Tonart von Musikdateien vorgestellt. Das Verfahren analysiert mittels einer Filterbank den Tonvorrat des Eingangssignals, der in einem Tonvektor zusammenfasst wird. Dabei sind sowohl mehrstimmige als auch ein- stimmige Eingangssignale zulässig. Mit Hilfe eines Nearest-Neighbour-Classifiers wird anschließ end das wahrscheinlichste Ergebnis für den extrahierten Tonvektor bestimmt. Parallel zur Analyse des Tonvorrats wird die Stimmhöhe des Kammertons detektiert, um eine gleichbleibende Erkennungsrate für Signale unterschiedlicher Stimmhöhe zu gewährleisten.},
	booktitle = {Proceedings of the {VDT} {International} {Audio} {Convention} (23. {Tonmeistertagung})},
	author = {Lerch, Alexander},
	month = nov,
	year = {2004},
	file = {Lerch - 2004 - Ein Ansatz zur automatischen Erkennung der Tonart .pdf:H\:\\Docs\\zotero\\storage\\WAM57AHA\\Lerch - 2004 - Ein Ansatz zur automatischen Erkennung der Tonart .pdf:application/pdf}
}

@article{kirchhoff_evaluation_2011,
	title = {Evaluation of {Features} for {Audio}-to-{Audio} {Alignment}},
	volume = {40},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/04/Kirchhoff_Lerch_2011_Evaluation%20of%20Features%20for%20Audio-to-Audio%20Alignment.pdf},
	doi = {10.1080/09298215.2010.529917},
	abstract = {Audio-to-audio alignment is the task of synchronizing two audio sequences with similar musical content in time. We investigated a large set of audio features for this task. The features were chosen to represent four different content-dependent similarity categories: the envelope, the timbre, note-onsets and the pitch. The features were subjected to two processing stages. First, a feature subset was selected by evaluating the alignment performance of each individual feature. Second, the selected features were combined and subjected to an automatic weighting algorithm.

A new method for the objective evaluation of audio-to-audio alignment systems is proposed that enables the use of arbitrary kinds of music as ground truth data. We evaluated our algorithm by this method as well as on a data set of real recordings of solo piano music. The results showed that the feature weighting algorithm could improve the alignment accuracies compared to the results of the individual features.},
	number = {1},
	journal = {Journal of New Music Research},
	author = {Kirchhoff, Holger and Lerch, Alexander},
	year = {2011},
	pages = {27--41},
	file = {Kirchhoff and Lerch - 2011 - Evaluation of Features for Audio-to-Audio Alignmen.pdf:H\:\\Docs\\zotero\\storage\\JX2SU8Z7\\Kirchhoff and Lerch - 2011 - Evaluation of Features for Audio-to-Audio Alignmen.pdf:application/pdf}
}

@phdthesis{lerch_untersuchung_2000,
	title = {Untersuchung und {Bewertung} psychoakustischer {Gehörmodelle}},
	school = {Technical University of Berlin},
	author = {Lerch, Alexander},
	year = {2000},
	file = {Lerch - 2000 - Untersuchung und Bewertung psychoakustischer Gehör.pdf:H\:\\Docs\\zotero\\storage\\2FDMJ22Q\\Lerch - 2000 - Untersuchung und Bewertung psychoakustischer Gehör.pdf:application/pdf}
}

@inproceedings{lu_unsupervised_2016,
	address = {Pisa},
	series = {{SIGIR} '16},
	title = {An {Unsupervised} {Approach} to {Anomaly} {Detection} in {Music} {Datasets}},
	isbn = {978-1-4503-4069-4},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/07/Lu-et-al_2016_An-Unsupervised-Approach-to-Anomaly-Detection-in-Music-Datasets.pdf},
	doi = {10.1145/2911451.2914700},
	abstract = {This paper presents an unsupervised method for systematically identifying anomalies in music datasets. The model integrates categorical regression and robust estimation techniques to infer anomalous scores in music clips. When applied to a music genre recognition dataset, the new method is able to detect corrupted, distorted, or mislabeled audio samples based on commonly used features in music information retrieval. The evaluation results show that the algorithm outperforms other anomaly detection methods and is capable of finding problematic samples identified by human experts. The proposed method introduces a preliminary framework for anomaly detection in music data that can serve as a useful tool to improve data integrity in the future.},
	booktitle = {Proceedings of the {ACM} {SIGIR} {Conference} ({SIGIR})},
	publisher = {ACM},
	author = {Lu, Yen-Cheng and Wu, Chih-Wei and Lu, Chang-Tien and Lerch, Alexander},
	year = {2016},
	keywords = {music information retrieval, anomaly detection, data clean-up, music genre retrieval},
	pages = {749--752},
	file = {Lu et al. - 2016 - An Unsupervised Approach to Anomaly Detection in M.pdf:H\:\\Docs\\zotero\\storage\\XJED5RM4\\Lu et al. - 2016 - An Unsupervised Approach to Anomaly Detection in M.pdf:application/pdf}
}

@misc{lerch_audio_2014,
	title = {Audio {Content} {Analysis}},
	url = {www.AudioContentAnalysis.org},
	urldate = {2014-06-18},
	author = {Lerch, Alexander},
	year = {2014},
	note = {last accessed: 2014-06-18}
}

@incollection{lerch_music_2014,
	series = {Handbuch der {Systematischen} {Musikwissenschaft}},
	title = {Music {Information} {Retrieval}},
	isbn = {978-3-89007-699-7},
	number = {5},
	booktitle = {Akustische {Grundlagen} der {Musik}},
	publisher = {Laaber},
	author = {Lerch, Alexander},
	editor = {Weinzierl, Stefan},
	year = {2014},
	pages = {79--102}
}

@inproceedings{wu_drum_2015,
	address = {Malaga},
	title = {Drum {Transcription} using {Partially} {Fixed} {Non}-{Negative} {Matrix} {Factorization} {With} {Template} {Adaptation}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/10/Wu_Lerch_2015_Drum-Transcription-using-Partially-Fixed-Non-Negative-Matrix-Factorization-With.pdf},
	abstract = {In this paper, a template adaptive drum transcription algo-
rithm using partially fixed Non-negative Matrix Factoriza-
tion (NMF) is presented. The proposed method detects per-
cussive events in complex mixtures of music with a minimal
training set. The algorithm decomposes the music signal
into two dictionaries: a percussive dictionary initialized
with pre-defined drum templates and a harmonic dictionary
initialized with undefined entries. The harmonic dictionary
is adapted to the non-percussive music content in a standard
NMF procedure. The percussive dictionary is adapted to
each individual signal in an iterative scheme: it is fixed
during the decomposition process, and is updated based on
the result of the previous convergence. Two template adap-
tation methods are proposed to provide more flexibility and
robustness in the case of unknown data. The performance
of the proposed system has been evaluated and compared
to state of the art systems. The results show that template
adaptation improves the transcription performance, and the
detection accuracy is in the same range as more complex
systems.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2015},
	file = {Wu and Lerch - 2015 - Drum Transcription using Partially Fixed Non-Negat.pdf:H\:\\Docs\\zotero\\storage\\T6Z73IFI\\Wu and Lerch - 2015 - Drum Transcription using Partially Fixed Non-Negat.pdf:application/pdf}
}

@inproceedings{lykartsis_beat_2015,
	address = {Malaga},
	title = {Beat {Histogram} {Features} from {NMF}-{Based} {Novelty} {Functions} for {Music} {Classification}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/10/Lykartsis-et-al_2015_Beat-Histogram-Features-from-NMF-Based-Novelty-Functions-for-Music.pdf},
	abstract = {In this paper we present novel rhythm features derived from
drum tracks extracted from polyphonic music and evaluate
them in a genre classification task. Musical excerpts are
analyzed using an optimized, partially fixed Non-Negative
Matrix Factorization (NMF) method and beat histogram
features are calculated on basis of the resulting activation
functions for each one out of three drum tracks extracted
(Hi-Hat, SnareDrumandBassDrum). Thefeaturesareeval-
uated on two widely used genre datasets (GTZAN and Ball-
room) using standard classification methods, concerning
the achieved overall classification accuracy. Furthermore,
their suitability in distinguishing between rhythmically sim-
ilar genres and the performance of the features resulting
from individual activation functions is discussed. Results
show that the presented NMF-based beat histogram features
can provide comparable performance to other classification
systems, while considering strictly drum patterns.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Lykartsis, Athanasios and Wu, Chih-Wei and Lerch, Alexander},
	year = {2015},
	file = {Lykartsis et al. - 2015 - Beat Histogram Features from NMF-Based Novelty Fun.pdf:H\:\\Docs\\zotero\\storage\\NDFBHMMP\\Lykartsis et al. - 2015 - Beat Histogram Features from NMF-Based Novelty Fun.pdf:application/pdf}
}

@inproceedings{zhou_chord_2015,
	address = {Malaga},
	title = {Chord {Detection} {Using} {Deep} {Learning}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/10/Zhou_Lerch_2015_Chord-Detection-Using-Deep-Learning.pdf},
	abstract = {In this paper, we utilize deep learning to learn high-level
features for audio chord detection. The learned features,
obtained by a deep network in bottleneck architecture, give
promising results and outperform state-of-the-art systems.
We present and evaluate the results for various methods and
configurations, including input pre-processing, a bottleneck
architecture, and SVMs vs. HMMs for chord classification.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Zhou, Xinquan and Lerch, Alexander},
	year = {2015},
	file = {Zhou and Lerch - 2015 - Chord Detection Using Deep Learning.pdf:H\:\\Docs\\zotero\\storage\\STZQS59K\\Zhou and Lerch - 2015 - Chord Detection Using Deep Learning.pdf:application/pdf;Zhou and Lerch - 2015 - Chord Detection Using Deep Learning.pdf:H\:\\Docs\\zotero\\storage\\BFICWEZP\\Zhou and Lerch - 2015 - Chord Detection Using Deep Learning.pdf:application/pdf}
}

@inproceedings{lykartsis_analysis_2015,
	address = {Nuremberg},
	title = {Analysis of {Speech} {Rhythm} for {Language} {Identification} {Based} on {Beat} {Histograms}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/06/Lykartsis%20et%20al_2015_Analysis%20of%20Speech%20Rhythm%20for%20Language%20Identification%20Based%20on%20Beat%20Histograms.pdf},
	booktitle = {Proceedings of the {DAGA} ({Jahrestagung} fur {Akustik})},
	author = {Lykartsis, Athanasios and Lerch, Alexander and Weinzierl, Stefan},
	year = {2015},
	file = {Lykartsis et al. - 2015 - Analysis of Speech Rhythm for Language Identificat.pdf:H\:\\Docs\\zotero\\storage\\IE6UA926\\Lykartsis et al. - 2015 - Analysis of Speech Rhythm for Language Identificat.pdf:application/pdf}
}

@inproceedings{gupta_perceptual_2015,
	address = {New Paltz},
	title = {On the {Perceptual} {Relevance} of {Objective} {Source} {Separation} {Measures} for {Singing} {Voice} {Separation}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/10/Gupta-et-al_2015_On-the-Perceptual-Relevance-of-Objective-Source-Separation-Measures-for-Singing.pdf},
	abstract = {Singing Voice Separation (SVS) is a task which uses audio source
separation methods to isolate the vocal component from the back-
ground accompaniment for a song mix. This paper discusses the
methods of evaluating SVS algorithms, and determines how the
current state of the art measures correlate to human perception. A
modified ITU-R BS.1543 MUSHRA test is used to get the human
perceptual ratings for the outputs of various SVS algorithms, which
are correlated with widely used objective measures for source sep-
aration quality. The results show that while the objective measures
provide a moderate correlation with perceived intelligibility and
isolation, they may not adequately assess the overall perceptual
quality.},
	booktitle = {Proceedings of the {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Gupta, Udit and Moore II, Elliot and Lerch, Alexander},
	year = {2015},
	file = {Gupta et al. - 2015 - On the Perceptual Relevance of Objective Source Se.pdf:H\:\\Docs\\zotero\\storage\\HC36DE7I\\Gupta et al. - 2015 - On the Perceptual Relevance of Objective Source Se.pdf:application/pdf}
}

@inproceedings{lykartsis_beat_2015-1,
	address = {Trondheim, Norway},
	title = {Beat {Histogram} {Features} for {Rhythm}-based {Musical} {Genre} {Classification} {Using} {Multiple} {Novelty} {Functions}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/12/DAFx-15_submission_42-1.pdf},
	abstract = {In this paper we present beat histogram features for multiple level
rhythmdescriptionandevaluatetheminamusicalgenreclassifica-
tion task. Audio features pertaining to various musical content cat-
egories and their related novelty functions are extracted as a basis
for the creation of beat histograms. The proposed features capture
not only amplitude, but also tonal and general spectral changes
in the signal, aiming to represent as much rhythmic information
as possible. The most and least informative features are identi-
fied through feature selection methods and are then tested using
Support Vector Machines on five genre datasets concerning classi-
fication accuracy against a baseline feature set. Results show that
the presented features provide comparable classification accuracy
with respect to other genre classification approaches using period-
icity histograms and display a performance close to that of much
more elaborate up-to-date approaches for rhythm description. The
use of bar boundary annotations for the texture frames has pro-
vided an improvement for the dance-oriented Ballroom dataset.
The comparably small number of descriptors and the possibility of
evaluating the influence of specific signal components to the gen-
eral rhythmic content encourage the further use of the method in
rhythm description tasks.},
	booktitle = {Proceedings of the {International} {Conference} on {Digital} {Audio} {Effects} ({DAFX})},
	author = {Lykartsis, Athanasios and Lerch, Alexander},
	year = {2015},
	file = {Lykartsis and Lerch - 2015 - Rhythm Features for Musical Genre Classification U.pdf:H\:\\Docs\\zotero\\storage\\WZP9NVDE\\Lykartsis and Lerch - 2015 - Rhythm Features for Musical Genre Classification U.pdf:application/pdf}
}

@inproceedings{wu_drum_2015-1,
	address = {Nice},
	title = {Drum {Transcription} using {Partially} {Fixed} {Non}-{Negative} {Matrix} {Factorization}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/09/Wu_Lerch_2015_Drum%20Transcription%20using%20Partially%20Fixed%20Non-Negative%20Matrix%20Factorization.pdf},
	abstract = {In this paper, a drum transcription algorithm using partially
fixed non-negative matrix factorization is presented. The pro-
posed method allows users to identify percussive events in
complex mixtures with a minimal training set. The algorithm
decomposes the music signal into two parts: percussive part
with pre-defined drum templates and harmonic part with un-
defined entries. The harmonic part is able to adapt to the
music content, allowing the algorithm to work in polyphonic
mixtures. Drum event times can be simply picked from the
percussive activation matrix with onset detection. The system
is efficient and robust even with a minimal training set. The
recognition rates for the ENST dataset vary from 56.7 to 78.9\%
for three percussive instruments extracted from polyphonic
music.},
	booktitle = {Proceedings of the {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {EURASIP},
	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2015},
	file = {Wu and Lerch - 2015 - Drum Transcription using Partially Fixed Non-Negat.pdf:H\:\\Docs\\zotero\\storage\\R4ZVUAXN\\Wu and Lerch - 2015 - Drum Transcription using Partially Fixed Non-Negat.pdf:application/pdf}
}

@inproceedings{obrien_genre-specific_2015,
	address = {Denton},
	title = {Genre-{Specific} {Key} {Profiles}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2015/09/O'Brien_Lerch_2015_Genre-Specific%20Key%20Profiles.pdf},
	abstract = {The most common approaches to the automatic recognition
of musical key are template-based, i.e., an extracted pitch
chroma vector is compared to a template key profile in order
to identify the most similar key. General as well as domain-
specific templates have been used in the past, but to the au-
thors best knowledge there has been no study that evaluated
genre-specific key profiles extracted from the audio signal. We
investigate the pitch chroma distributions for 9 different gen-
res, their distances, and the degree to which these genres can
be identified using these distributions when utilizing different
strategies for achieving key-invariance.},
	booktitle = {Proceedings of the {International} {Computer} {Music} {Conference} ({ICMC})},
	publisher = {ICMA},
	author = {O'Brien, Cian and Lerch, Alexander},
	year = {2015},
	file = {O'Brien and Lerch - 2015 - Genre-Specific Key Profiles.pdf:H\:\\Docs\\zotero\\storage\\K9IEZGMB\\O'Brien and Lerch - 2015 - Genre-Specific Key Profiles.pdf:application/pdf}
}

@misc{baumgarte_delayed_2001,
	title = {Delayed {Contribution} {Document} 6Q/18-{E}: {Implementation} of {Recommendation} {ITU}-{R} {BS}.1387 ({PEAQ})},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/10/Baumgarte-and-Lerch-2001-Delayed-Contribution-Document-6Q18-E-Implementat.pdf},
	publisher = {ITU},
	author = {Baumgarte, Frank and Lerch, Alexander},
	year = {2001},
	file = {Baumgarte and Lerch - 2001 - Delayed Contribution Document 6Q18-E Implementat.pdf:H\:\\Docs\\zotero\\storage\\IZD2H2CI\\Baumgarte and Lerch - 2001 - Delayed Contribution Document 6Q18-E Implementat.pdf:application/pdf;Baumgarte_Lerch_2001_Delayed Contribution Document 6Q-18-E.DOC:H\:\\Docs\\zotero\\storage\\37CEPPZH\\Baumgarte_Lerch_2001_Delayed Contribution Document 6Q-18-E.DOC:application/msword}
}

@book{freeman_proceedings_2016,
	address = {Atlanta},
	title = {Proceedings of the 2nd {Web} {Audio} {Conference} ({WAC}-2016)},
	isbn = {978-0-692-61973-5},
	url = {https://smartech.gatech.edu/handle/1853/54577},
	publisher = {Georgia Institute of Technology},
	editor = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
	year = {2016}
}

@inproceedings{winters_automatic_2016,
	address = {New York},
	title = {Automatic {Practice} {Logging}: {Introduction}, {Dataset} \& {Preliminary} {Study}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/07/Winters-et-al_2016_Automatic-Practice-Logging.pdf},
	abstract = {Musicians spend countless hours practicing their instru-
ments. To document and organize this time, musicians com-
monly use practice charts to log their practice. However,
manual techniques require time, dedication, and experience
to master, are prone to fallacy and omission, and ultimately
can not describe the subtle variations in each repetition.
This paper presents an alternative: by analyzing and clas-
sifying the audio recorded while practicing, logging could
occur automatically, with levels of detail, accuracy, and ease
that would not be possible otherwise. Towards this goal,
we introduce the problem of Automatic Practice Logging
(APL), including a discussion of the benefits and unique
challenges it raises. We then describe a new dataset of over
600 annotated recordings of solo piano practice, which can
be used to design and evaluate APL systems. After fram-
ing our approach to the problem, we present an algorithm
designed to align short segments of practice audio with
reference recordings using pitch chroma and dynamic time
warping.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Winters, R Michael and Gururani, Siddharth and Lerch, Alexander},
	year = {2016},
	file = {Winters et al. - 2016 - Automatic Practice Logging Introduction, Dataset .pdf:H\:\\Docs\\zotero\\storage\\CDVQRU96\\Winters et al. - 2016 - Automatic Practice Logging Introduction, Dataset .pdf:application/pdf}
}

@inproceedings{wu_towards_2016,
	address = {San Francisco},
	title = {Towards the {Objective} {Assessment} of {Music} {Performances}},
	isbn = {1-879346-65-5},
	abstract = {The qualitative assessment of music performances is a task that is
influenced by technical correctness, deviations from established
performance standards, and aesthetic judgment. Despite its inherently
subjective nature, a quantitative overall assessment is often desired,
as exemplified by US all-state auditions or other competitions. A
model that automatically generates assessments from the audio data
would allow for objective assessments and enable musically
intelligent computer-assisted practice sessions for students learning
an instrument. While existing systems are already able to provide
similar basic functionality, they rely on the musical score as prior
knowledge. In this paper, we present a score-independent system for
assessing student instrument performances based on audio recordings.
This system aims to characterize the performance with both
well-established and custom-designed audio features, model expert
assessments of student performances, and predict the assessment of
unknown audio recordings. The results imply the viability of
modeling human assessment with score-independent audio features.
Results could lead towards more general software music tutoring
systems that do not require score information for the assessment of
student music performances.},
	booktitle = {{ICMPC}},
	author = {Wu, Chih-Wei and Gururani, Siddharth and Laguna, Christopher and Pati, Ashis and Vidwans, Amruta and Lerch, Alexander},
	year = {2016},
	pages = {99--103},
	file = {Wu et al. - 2016 - Towards the Objective Assessment of Music Performa.pdf:H\:\\Docs\\zotero\\storage\\CKW7JZ9U\\Wu et al. - 2016 - Towards the Objective Assessment of Music Performa.pdf:application/pdf;Wu et al. - 2016 - Towards the Objective Assessment of Music Performa.pdf:H\:\\Docs\\zotero\\storage\\QM75CNV5\\Wu et al. - 2016 - Towards the Objective Assessment of Music Performa.pdf:application/pdf}
}

@inproceedings{laguna_efficient_2016,
	address = {Los Angeles},
	title = {An {Efficient} {Algorithm} {For} {Clipping} {Detection} {And} {Declipping} {Audio}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/09/Laguna_Lerch_2016_An-Efficient-Algorithm-For-Clipping-Detection-And-Declipping-Audio.pdf},
	abstract = {We present an algorithm for end to end declipping, which includes clipping detection and the replacement
of clipped samples. To detect regions of clipping, we analyze the signal’s amplitude histogram and the
shape of the signal in the time-domain. The sample replacement algorithm uses a two-pass approach: short
regions of clipping are replaced in the time-domain and long regions of clipping are replaced in the
frequency-domain. The algorithm is robust against different types of clipping and is efficient compared to
existing approaches. The algorithm has been implemented in an open source JavaScript client-side web
application. Clipping detection is shown to give an f-measure of 0.92 and is robust to the clipping level.},
	booktitle = {Proceedings of the 141st {AES} {Convention}},
	publisher = {Audio Engineering Society (AES)},
	author = {Laguna, Christopher and Lerch, Alexander},
	year = {2016},
	file = {Laguna and Lerch - 2016 - An Efficient Algorithm For Clipping Detection And .pdf:H\:\\Docs\\zotero\\storage\\P45VCX2D\\Laguna and Lerch - 2016 - An Efficient Algorithm For Clipping Detection And .pdf:application/pdf}
}

@inproceedings{wu_drum_2016,
	address = {New York},
	title = {On {Drum} {Playing} {Technique} {Detection} in {Polyphonic} {Mixtures}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/07/Wu_Lerch_2016_On-Drum-Playing-Technique-Detection-in-Polyphonic-Mixtures.pdf},
	abstract = {In this paper, the problem of drum playing technique
detection in polyphonic mixtures of music is addressed.
We focus on the identification of 4 rudimentary techniques:
strike, buzz roll, flam, and drag. The specifics and the
challenges of this task are being discussed, and different
sets of features are compared, including various features
extracted from NMF-based activation functions, as well
as baseline spectral features. We investigate the capabil-
ities and limitations of the presented system in the case
of real-world recordings and polyphonic mixtures. To de-
sign and evaluate the system, two datasets are introduced: a
training dataset generated from individual drum hits, and ad-
ditional annotations of the well-known ENST drum dataset
minus one subset as test dataset. The results demonstrate
issues with the traditionally used spectral features, and in-
dicate the potential of using NMF activation functions for
playing technique detection, however, the performance of
polyphonic music still leaves room for future improvement.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2016},
	file = {Wu and Lerch - 2016 - On Drum Playing Technique Detection in Polyphonic .pdf:H\:\\Docs\\zotero\\storage\\AKTIRG9Z\\Wu and Lerch - 2016 - On Drum Playing Technique Detection in Polyphonic .pdf:application/pdf}
}

@inproceedings{lu_automatic_2016,
	address = {New York},
	series = {{ISMIR}},
	title = {Automatic {Outlier} {Detection} in {Music} {Genre} {Datasets}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/07/Lu-et-al_2016_Automatic-Outlier-Detection-in-Music-Genre-Datasets.pdf},
	abstract = {Outlier detection, also known as anomaly detection, is an
importanttopicthathasbeenstudiedfordecades. Anoutlier
detection system is able to identify anomalies in a dataset
and thus improve data integrity by removing the detected
outliers. It has been successfully applied to different types
of data in various fields such as cyber-security, finance,
and transportation. In the field of Music Information Re-
trieval (MIR), however, the number of related studies is
small. In this paper, we introduce different state-of-the-art
outlier detection techniques and evaluate their viability in
the context of music datasets. More specifically, we present
a comparative study of 6 outlier detection algorithms ap-
plied to a Music Genre Recognition (MGR) dataset. It is
determined how well algorithms can identify mislabeled or
corrupted files, and how much the quality of the dataset can
be improved. Results indicate that state-of-the-art anomaly
detection systems have problems identifying anomalies in
MGR datasets reliably.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Lu, Yen-Cheng and Wu, Chih-Wei and Lu, Chang-Tien and Lerch, Alexander},
	year = {2016},
	keywords = {music information retrieval, anomaly detection, data clean-up, music genre retrieval},
	file = {Lu et al. - 2016 - Automatic Outlier Detection in Music Genre Dataset.pdf:H\:\\Docs\\zotero\\storage\\848ZE7II\\Lu et al. - 2016 - Automatic Outlier Detection in Music Genre Dataset.pdf:application/pdf}
}

@inproceedings{xambo_learning_2016,
	address = {New York},
	title = {Learning to code through {MIR}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2016/08/Xambo-et-al.-2016-Learning-to-code-through-MIR.pdf},
	abstract = {An approach to teaching computer science (CS) in high-
schools is using EarSketch, a free online tool for teaching
CS concepts while making music. In this demonstration we
present the potential of teaching music information retrieval
(MIR) concepts using EarSketch. The aim is twofold: to
discuss the benefits of introducing MIR concepts in the
classroom and to shed light on how MIR concepts can
be gently introduced in a CS curriculum. We conclude by
identifying the advantagesofteachingMIR inthe classroom
and pointing to future directions for research.},
	booktitle = {Late {Breaking} {Demo} ({Extended} {Abstract}), {Proceedings} of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {ISMIR},
	author = {Xambo, Anna and Lerch, Alexander and Freeman, Jason},
	year = {2016},
	file = {Xambo et al. - 2016 - Learning to code through MIR.pdf:H\:\\Docs\\zotero\\storage\\HDRRR8X4\\Xambo et al. - 2016 - Learning to code through MIR.pdf:application/pdf}
}

@inproceedings{vidwans_objective_2017,
	address = {Erlangen},
	title = {Objective descriptors for the assessment of student music performances},
	abstract = {Assessment of students’ music performances is a subjective task that requires the judgment of technical correctness as well as aesthetic properties. A computational model automatically evaluating music performance based on objective measurements could ensure consistent and reproducible assessments for, e.g., automatic music tutoring systems. In this study, we investigate the effectiveness of various audio descriptors for assessing performances. Specifically, three different sets of features, including a baseline set, score-independent features, and score-based features, are compared with respect to their efficiency in regression tasks. The results show that human assessments can be modeled to a certain degree, however, the generality of the model still needs further investigation.},
	booktitle = {{AES} {Conference} on {Semantic} {Audio}},
	author = {Vidwans, Amruta and Gururani, Siddharth and Wu, Chih-Wei and Subramanian, Vinod and Swaminathan, Rupak Vignesh and Lerch, Alexander},
	year = {2017},
	keywords = {computational auditory scene analysis, Computer sound processing, Content analysis (Communication), Data processing},
	file = {Vidwans et al_2017_Objective descriptors for the assessment of student music performances.pdf:H\:\\Docs\\zotero\\storage\\TIJ639H7\\Vidwans et al_2017_Objective descriptors for the assessment of student music performances.pdf:application/pdf}
}

@inproceedings{pati_dataset_2017,
	address = {Erlangen},
	title = {A {Dataset} and {Method} for {Electric} {Guitar} {Solo} {Detection} in {Rock} {Music}},
	abstract = {This paper explores the problem of automatically detecting electric guitar solos in rock music. A baseline study using standard spectral and temporal audio features in conjunction with an SVM classifier is carried out. To improve detection rates, custom features based on predominant pitch and structural segmentation of songs are designed and investigated. The evaluation of different feature combinations suggests that the combination of all features followed by a post-processing step results in the best accuracy. A macro-accuracy of 78.6\% with a solo detection precision of 63.3\% is observed for the best feature combination. This publication is accompanied by release of an annotated dataset of electric guitar solos to encourage future research in this area.},
	booktitle = {Proceedings of the {AES} {Conference} on {Semantic} {Audio}},
	publisher = {Audio Engineering Society (AES)},
	author = {Pati, Kumar Ashis and Lerch, Alexander},
	year = {2017},
	file = {Pati_Lerch_2017_A Dataset and Method for Electric Guitar Solo Detection in Rock Music.pdf:H\:\\Docs\\zotero\\storage\\96I7HMV8\\Pati_Lerch_2017_A Dataset and Method for Electric Guitar Solo Detection in Rock Music.pdf:application/pdf}
}

@inproceedings{gururani_automatic_2017,
	address = {Suzhou},
	title = {Automatic {Sample} {Detection} in {Polyphonic} {Music}},
	abstract = {The term `sampling' refers to the usage of snippets or loops from existing songs or sample libraries in new songs, mashups, or other music productions. The ability to automatically detect sampling in music is, for instance, beneficial for studies tracking artist influences geographically and temporally. We present a method based on Non-negative Matrix Factorization (NMF) and Dynamic Time Warping (DTW) for the automatic detection of a sample in a pool of songs. The method comprises of two processing steps: first, the DTW alignment path between NMF activations of a song and query sample is computed. Second, features are extracted from this path and used to train a Random Forest classifier to detect the presence of the sample. The method is able to identify samples that are pitch shifted and/or time stretched with approximately 63\% F-measure. We evaluate this method against a new publicly available dataset of real-world sample and song pairs.},
	booktitle = {{ISMIR}},	author = {Gururani, Siddharth and Lerch, Alexander},
	year = {2017},
}

@inproceedings{wu_automatic_2017,
	address = {Suzhou},
	title = {Automatic drum transcription using the student-teacher learning paradigm with unlabeled music data},
	abstract = {Automatic drum transcription is a sub-task of automatic music transcription that converts drum-related audio events into musical notation. While noticeable progress has been made in the past by combining pattern recognition methods with audio signal processing techniques, the major limitation of many state-of-the-art systems still originates from the difficulty of obtaining a meaningful amount of annotated data to support the data-driven algorithms. In this work, we address the challenge of insufficiently labeled data by exploring the possibility of utilizing unlabeled music data from online resources. Specifically, a student neural network is trained using the labels generated from multiple teacher systems. The performance of the model is evaluated on a publicly available dataset. The results show the general viability of using unlabeled music data to improve the performance of drum transcription systems.},
booktitle = {{ISMIR}},	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2017},
	file = {Wu_Lerch_2017_Automatic drum transcription using the student-teacher learning paradigm with.pdf:H\:\\Docs\\zotero\\storage\\T5WQFKDS\\Wu_Lerch_2017_Automatic drum transcription using the student-teacher learning paradigm with.pdf:application/pdf}
}

@inproceedings{zhiqian_chen_learning_2017,
	address = {New Orleans},
	title = {Learning to {Fuse} {Music} {Genres} with {Generative} {Adversarial} {Dual} {Learning}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2017/11/Zhiqian-Chen-et-al_2017_Learning-to-Fuse-Music-Genres-with-Generative-Adversarial-Dual-Learning.pdf},
	abstract = {FusionGAN is a novel genre fusion framework for music generation that integrates the strengths of generative adversarial networks and dual learning. In particular, the proposed method offers a dual learning extension that can effectively integrate the styles of the given domains. To efficiently quantify the difference among diverse domains and avoid the vanishing gradient issue, FusionGAN provides a Wasserstein based metric to approximate the distance between the target domain and the existing domains. Adopting the Wasserstein distance, a new domain is created by combining the patterns of the existing domains using adversarial learning. Experimental results on public music datasets demonstrated that our approach could effectively merge two genres.},
	booktitle = {Proceedings of the {International} {Conference} on {Data} {Mining} ({ICDM})},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {{Zhiqian Chen} and Wu, Chih-Wei and Lu, Yen-Cheng and Lerch, Alexander and Lu, Chang-Tien},
	year = {2017},
	file = {Zhiqian Chen et al_2017_Learning to Fuse Music Genres with Generative Adversarial Dual Learning.pdf:H\:\\Docs\\zotero\\storage\\PINJ7KMC\\Zhiqian Chen et al_2017_Learning to Fuse Music Genres with Generative Adversarial Dual Learning.pdf:application/pdf}
}

@inproceedings{southall_mdb_2017,
	address = {Suzhou},
	title = {{MDB} {Drums} --- {An} {Annotated} {Subset} of {MedleyDB} for {Automatic} {Drum} {Transcription}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2017/10/Wu-et-al_2017_MDB-Drums-An-Annotated-Subset-of-MedleyDB-for-Automatic-Drum-Transcription.pdf},
	abstract = {In this paper we present MDB Drums, a new dataset for automatic drum transcription (ADT) tasks. This dataset is built on top of the MusicDelta subset of the MedleyDB dataset, taking advantage of real-world recordings in multi-track format. The dataset is comprised of a variety of genres, providing a balanced pool for developing and evaluating ADT models with respect to various musical styles.
To reduce the cost of the labor-intensive process of manual annotation, a semi-automatic process was utilised in both the annotation and quality control processes. The pre sented dataset consists of 23 tracks with a total of 7994 onsets. These onsets are divided into 6 classes based on drum instruments or 21 subclasses based on playing techniques. Every track consists of a drum-only track as well
as multiple accompanied tracks, enabling audio files containing different combinations of instruments to be used in the ADT evaluation process.},
	booktitle = {Late {Breaking} {Demo} ({Extended} {Abstract}), {Proceedings} of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {International Society for Music Information Retrieval (ISMIR)},
	author = {Southall, Carl and Wu, Chih-Wei and Lerch, Alexander and Hockman, Jason A},
	year = {2017},
	file = {Southall et al_2017_MDB Drums --- An Annotated Subset of MedleyDB for Automatic Drum Transcription.pdf:H\:\\Docs\\zotero\\storage\\BEQV2VGE\\Southall et al_2017_MDB Drums --- An Annotated Subset of MedleyDB for Automatic Drum Transcription.pdf:application/pdf}
}

@inproceedings{gururani_mixing_2017,
	address = {Suzhou},
	title = {Mixing {Secrets}: {A} multitrack dataset for instrument detection in polyphonic music},
	abstract = {Instrument recognition as a task in MIR is largely data drive. This drives a need for large datasets that cater to the need of these algorithms. Several datasets exist for the task of instrument recognition in monophonic signals. For polyphonic music, creating a finely labeled dataset for instrument recognition is a hard task and using multi-track data eases that process. We present 250+ multi-tracks that have been labeled for instrument recognition and release the annotations to be used in the community. The process of data acquisition, cleaning and labeling has been detailed in this late-breaking demo.},
	booktitle = {Late {Breaking} {Demo} ({Extended} {Abstract}), {Proceedings} of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	publisher = {International Society for Music Information Retrieval (ISMIR)},
	author = {Gururani, Siddharth and Lerch, Alexander},
	year = {2017},
	file = {Gururani_Lerch_2017_Mixing Secrets.pdf:H\:\\Docs\\zotero\\storage\\3UQHK96Q\\Gururani_Lerch_2017_Mixing Secrets.pdf:application/pdf}
}

@article{wu_review_2018,
	title = {A {Review} of {Automatic} {Drum} {Transcription}},
	volume = {26},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2018.2830113},
	abstract = {In Western popular music, drums and percussion are an important means to emphasize and shape the rhythm, often defining the musical style. If computers were able to analyze the drum part in recorded music, it would enable a variety of rhythm-related music processing tasks. Especially the detection and classification of drum sound events by computational methods is considered to be an important and challenging research problem in the broader field of Music Information Retrieval. Over the last two decades, several authors have attempted to tackle this problem under the umbrella term Automatic Drum Transcription (ADT). This paper presents a comprehensive review of ADT research, including a thorough discussion of the task-specific challenges, categorization of existing techniques, and evaluation of several state-of-the-art systems. To provide more insights on the practice of ADT systems, we focus on two families of ADT techniques, namely methods based on Non-negative Matrix Factorization and Recurrent Neural Networks. We explain the methods' technical details and drum-specific variations and evaluate these approaches on publicly available datasets with a consistent experimental setup. Finally, the open issues and under-explored areas in ADT research are identified and discussed, providing future directions in this field.},
	number = {9},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wu, Chih-Wei and Dittmar, Christian and Southall, Carl and Vogl, Richard and Widmer, Gerhard and Hockman, Jason A and Muller, Meinard and Lerch, Alexander},
	year = {2018},
	keywords = {Instruments, Rhythm, Spectrogram, Transient analysis, Speech processing, Automatic Drum Transcription, Automatic Music Transcription, Deep learning, Machine Learning, Matrix Factorization, Music Information Retrieval, Task analysis},
	pages = {1457--1483},
	file = {08350302.pdf:H\:\\Docs\\zotero\\storage\\EWZQLIMV\\08350302.pdf:application/pdf;IEEE Xplore Abstract Record:H\:\\Docs\\zotero\\storage\\IZMMG57E\\8350302.html:text/html;IEEE Xplore Full Text PDF:H\:\\Docs\\zotero\\storage\\ECITUVIB\\Wu et al. - 2018 - A review of automatic drum transcription.pdf:application/pdf}
}

@inproceedings{wu_learned_2018,
	address = {Laguna Hills},
	title = {Learned {Features} for the {Assessment} of {Percussive} {Music} {Performances}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2018/01/Wu_Lerch_2018_Learned-Features-for-the-Assessment-of-Percussive-Music-Performances.pdf},
	abstract = {The automatic assessment of (student) music performance involves the characterization of the audio recordings and the modeling of human judgments. To build a computational model that provides a reliable assessment, the system must take into account various aspects of a performance including technical correctness and aesthetic standards. While some progress has been made in recent years, the search for an effective feature representation remains open-ended. In this study, we explore the possibility of using learned features from sparse coding. Specifically, we investigate three sets of features, namely a baseline set, a set of designed features, and a feature set learned with sparse coding. In addition, we compare the impact of two different input representations on the effectiveness of the learned features. The evaluation is performed on a dataset of annotated recordings of students playing snare exercises. The results imply the general viability of feature learning in the context of automatic assessment of music performances.},
	booktitle = {Proceedings of the {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	publisher = {IEEE},
	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2018},
	file = {Wu_Lerch_2018_Learned Features for the Assessment of Percussive Music Performances.pdf:H\:\\Docs\\zotero\\storage\\U9X9GS2U\\Wu_Lerch_2018_Learned Features for the Assessment of Percussive Music Performances.pdf:application/pdf}
}

@inproceedings{seipel_multi-track_2018,
	address = {Milan},
	title = {Multi-{Track} {Crosstalk} {Reduction}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2018/06/Seipel-and-Lerch-2018-Multi-Track-Crosstalk-Reduction.pdf},
	abstract = {While many music-related blind source separation methods focus on mono or stereo material, the detection and reduction of crosstalk in multi-track recordings is less researched. Crosstalk or ’bleed’ of one recorded channel in another is a very common phenomenon in specific genres such as jazz and classical, where all instrumentalists are recorded simultaneously. We present an efficient algorithm that estimates the crosstalk amount in the spectral domain and applies spectral subtraction to remove it. Randomly generated artificial mixtures from various anechoic orchestral source material were employed to develop and evaluate the algorithm, which scores an average SIR-Gain result of 15.14dB on various datasets with different amounts of simulated crosstalk.},
	booktitle = {Proceedings of the {Audio} {Engineering} {Society} {Convention}},
	publisher = {Audio Engineering Society (AES)},
	author = {Seipel, Fabian and Lerch, Alexander},
	year = {2018},
	file = {Seipel and Lerch - 2018 - Multi-Track Crosstalk Reduction.pdf:H\:\\Docs\\zotero\\storage\\U28N7ES4\\Seipel and Lerch - 2018 - Multi-Track Crosstalk Reduction.pdf:application/pdf}
}

@incollection{lerch_relation_2018,
	series = {Springer {Handbooks}},
	title = {The {Relation} {Between} {Music} {Technology} and {Music} {Industry}},
	isbn = {978-3-662-55002-1 978-3-662-55004-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-662-55004-5_44},
	abstract = {The music industry has changed drastically over the last century and most of its changes and transformations have been technology-driven. Music technology – encompassing musical instruments, sound generators, studio equipment and software, perceptual audio coding algorithms, and reproduction software and devices – has shaped the way music is produced, performed, distributed, and consumed. The evolution of music technology enabled studios and hobbyist producers to produce music at a technical quality unthinkable decades ago and have affordable access to new effects as well as production techniques. Artists explore nontraditional ways of sound generation and sound modification to create previously unheard effects, soundscapes, or even to conceive new musical styles. The consumer has immediate access to a vast diversity of songs and styles and is able to listen to individualized playlists virtually everywhere and at any time. The most disruptive technological innovations during the past 130 years have probably been:1. The possibility to record and distribute recordings on a large scale through the gramophone. 2. The introduction of vinyl disks enabling high-quality sound reproduction. 3. The compact cassette enabling individualized playlists, music sharing with friends and mobile listening. 4. Digital audio technology enabling high quality professional-grade studio equipment at low prices. 5. Perceptual audio coding in combination with online distribution, streaming, and file sharing. This text will describe these technological innovations and their impact on artists, engineers, and listeners.},
	language = {en},
	urldate = {2018-03-26},
	booktitle = {Springer {Handbook} of {Systematic} {Musicology}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Lerch, Alexander},
	editor = {Bader, Rolf},
	year = {2018},
	doi = {10.1007/978-3-662-55004-5_44},
	pages = {899--909},
	file = {Lerch - 2018 - The Relation Between Music Technology and Music In.pdf:H\:\\Docs\\zotero\\storage\\3UFDDLCL\\Lerch - 2018 - The Relation Between Music Technology and Music In.pdf:application/pdf;Snapshot:H\:\\Docs\\zotero\\storage\\LAJDA7TG\\978-3-662-55004-5_44.html:text/html}
}

@article{pati_assessment_2018,
	title = {Assessment of {Student} {Music} {Performances} {Using} {Deep} {Neural} {Networks}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	abstract = {Music performance assessment is a highly subjective task often relying on experts to gauge both the technical and aesthetic aspects of the performance from the audio signal. This article explores the task of building computational models for music performance assessment, i.e., analyzing an audio recording of a performance and rating it along several criteria such as musicality, note accuracy, etc. Much of the earlier work in this area has been centered around using hand-crafted features intended to capture relevant aspects of a performance. However, such features are based on our limited understanding of music perception and may not be optimal. In this article, we propose using Deep Neural Networks (DNNs) for the task and compare their performance against a baseline model using standard and hand-crafted features. We show that, using input representations at different levels of abstraction, DNNs can outperform the baseline models across all assessment criteria. In addition, we use model analysis techniques to further explain the model predictions in an attempt to gain useful insights into the assessment process. The results demonstrate the potential of using supervised feature learning techniques to better characterize music performances.},
	number = {4},
	journal = {Applied Sciences},
	author = {Pati, Kumar Ashis and Gururani, Siddharth and Lerch, Alexander},
	year = {2018},
	keywords = {music information retrieval, MIR, deep neural networks, deep learning, DNN, music education, music informatics, music learning, music performance assessment},
	pages = {507},
}

@inproceedings{laguna_client-side_2016,
	address = {Atlanta},
	title = {Client-{Side} {Audio} {Declipping}},
	copyright = {Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0).},
	isbn = {978-0-692-61973-5},
	url = {https://smartech.gatech.edu/handle/1853/54629},
	abstract = {Clipping is an unpleasant recording artifact that occurs when an audio signal’s level rises above a microphone’s or AD converter’s 
maximum input level. As more audio and video recordings are being taken on mobile devices (sometimes in high sound level conditions 
such as live concerts), clipping has become an issue that users encounter frequently. We present ClipAway: a web application that analyzes an audio file and automatically removes clipping from the audio file. 
The following scenario illustrates the service we supply: 1) A user attends a live concert and creates an audio recording of the concert. 2) 
The user listens to the recording at home and notices clipping, which causes the listening experience to be unsatisfactory. 3) The user 
uploads the audio recording to the ClipAway website. 4) Audio processing occurs in the browser, and the user then exports a qualityenhanced 
version of the recording. 5) The listening experience with the resulting audio file has significantly improved. 
The advantage of browser-based processing is that it is more familiar and accessible to most users than a native solution, which would 
likely require the user to install a standalone software or a digital audio workstation hosting a plugin for quality enhancement. 
The declipping algorithm is split into two sections: clipping detection and clipping correction. Clipping detection involves the automatic 
estimation of the clipping level and the subsequent localization of clipped regions. The clipping level is determined by identifying 
anomalies in the signal’s amplitude histogram near the positive and negative endpoints. The locations of clipping are determined by 
identifying samples with amplitudes close to the clipping level where the signal has a near-horizontal slope. Clipping correction involves 
replacing short clipped regions using spline interpolation and replacing long clipped regions through linear interpolation of time-frequency 
bin magnitudes.},
	language = {en},
	urldate = {2018-03-30},
	booktitle = {Proceedings of the {Web} {Audio} {Conference} ({WAC})},
	publisher = {Georgia Institute of Technology},
	author = {Laguna, Christopher and Lerch, Alexander},
	month = apr,
	year = {2016},
	file = {Full Text PDF:H\:\\Docs\\zotero\\storage\\B4ZT7KCE\\Laguna and Lerch - 2016 - Client-Side Audio Declipping.pdf:application/pdf;Snapshot:H\:\\Docs\\zotero\\storage\\78ZEASEJ\\54629.html:text/html}
}

@misc{lerch_tutorial_2015,
	address = {Atlanta},
	type = {Celebrating {Teaching} {Day}, {Georgia} {Institute} of {Technology}},
	title = {Tutorial videos produced by students},
	author = {Lerch, Alexander},
	year = {2015}
}

@inproceedings{xambo_live_2018,
	address = {Blacksburg},
	title = {Live {Repurposing} of {Sounds}: {MIR} {Explorations} with {Personal} and {Crowd}-sourced {Databases}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2018/04/Xambo-et-al.-2018-Live-Repurposing-of-Sounds-MIR-Explorations-with-.pdf},
	abstract = {The recent increase in the accessibility and size of personal and crowd-sourced digital sound collections brought about a valuable resource for music creation. Finding and retrieving relevant sounds in performance leads to challenges that can be approached using music information retrieval (MIR). In this paper, we explore the use of MIR to retrieve and repurpose sounds in musical live coding. We present a live coding system built on SuperCollider enabling the use of audio content from Creative Commons (CC) sound databases such as Freesound or personal sound databases. The novelty of our approach lies in exploiting high-level MIR methods (e.g. query by pitch or rhythmic cues) using live coding techniques applied to sounds. We demonstrate its potential through the reflection of an illustrative case study and the feedback from four expert users. The users tried the system with either a personal database or a crowd-source database and reported its potential in facilitating tailorability of the tool to their own creative workflows. This approach to live repurposing of sounds can be applied to real-time interactive systems for performance and composition beyond live coding, as well as inform live coding and MIR research.},
	booktitle = {Proceedings of the {Conference} on {New} {Interfaces} for {Musical} {Expression} ({NIME})},
	author = {Xambo, Anna and Roma, Gerard and Lerch, Alexander and Barthet, Matthieu and Fazekas, Gyorgy},
	year = {2018},
	file = {Xambo et al. - 2018 - Live Repurposing of Sounds MIR Explorations with .pdf:H\:\\Docs\\zotero\\storage\\LCZKB22L\\Xambo et al. - 2018 - Live Repurposing of Sounds MIR Explorations with .pdf:application/pdf}
}


@article{wu_assessment_2018,
	title = {Assessment of {Percussive} {Music} {Performances} with {Feature} {Learning}},
	volume = {12},
	number = {3},
	journal = {IJSC},
	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2018},
	pages = {315--333}
}

@article{xambo_music_nodate,
	title = {Music {Information} {Retrieval} in {Live} {Coding}: {A} {Theoretical} {Framework}},
	journal = {Computer Music Journal},
	author = {Xambo, Anna and Freeman, Jason and Lerch, Alexander}
}

@article{yang_evaluation_2018,
	title = {Remixing {Music} with {Visual} {Conditioning}},
	journal = {in review},
	author = {Yang, Li-Chia and Lerch, Alexander}
}

@inproceedings{subramanian_concert_2018,
	address = {Paris},
	title = {Concert {Stitch}: {Organization} and {Synchronization} of {Crowd}-{Sourced} {Recordings}},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Subramanian, Vinod and Lerch, Alexander},
	year = {2018},
	file = {Subramanian and Lerch - Concert Stitch Organization and Synchromization o.pdf:H\:\\Docs\\zotero\\storage\\CJGH6KI4\\Subramanian and Lerch - Concert Stitch Organization and Synchromization o.pdf:application/pdf}
}

@inproceedings{wu_labeled_2018,
	address = {Paris},
	title = {From {Labeled} to {Unlabeled} {Data} -- {On} the {Data} {Challenge} in {Automatic} {Drum} {Transcription}},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Wu, Chih-Wei and Lerch, Alexander},
	year = {2018},
	file = {Wu and Lerch - From Labeled to Unlabeled Data -- On the Data Chal.pdf:H\:\\Docs\\zotero\\storage\\RIMR7JYL\\Wu and Lerch - From Labeled to Unlabeled Data -- On the Data Chal.pdf:application/pdf}
}

@inproceedings{gururani_instrument_2018,
	address = {Paris},
	title = {Instrument {Activity} {Detection} in {Polyphonic} {Music} using {Deep} {Neural} {Networks}},
	booktitle = {{ISMIR}},
	author = {Gururani, Siddharth and Summers, Cameron and Lerch, Alexander},
	year = {2018},
	file = {Gururani et al. - Instrument Activity Detection in Polyphonic Music .pdf:H\:\\Docs\\zotero\\storage\\B7CI5UXR\\Gururani et al. - Instrument Activity Detection in Polyphonic Music .pdf:application/pdf}
}

@inproceedings{gururani_analysis_2018,
	address = {Toronto, Ontario, Canada},
	title = {Analysis of {Objective} {Descriptors} for {Music} {Performance} {Assessment}},
	abstract = {The assessment of musical performances in, e.g., student competitions or auditions, is a largely subjective evaluation of a performer's technical skills and expressivity. Objective descriptors extracted from the audio signal have been proposed for automatic performance assessment in such a context. Such descriptors represent different aspects of pitch, dynamics and timing of a performance and have been shown to be reasonably successful in modeling human assessments of student performances through regression. This study aims to identify the influence of individual descriptors on models of human assessment in 4 categories: musicality, note accuracy, rhythmic accuracy, and tone quality. To evaluate the influence of the individual descriptors, the descriptors highly correlated with the human assessments are identified. Subsequently, various subsets are chosen using different selection criteria and the adjusted R-squared metric is computed to evaluate the degree to which these subsets explain the variance in the assessments. In addition, sequential forward selection is performed to identify the most meaningful descriptors. The goal of this study is to gain insights into which objective descriptors contribute most to the human assessments as well as to identify a subset of well-performing descriptors. The results indicate that a small subset of the designed descriptors can perform at a similar accuracy as the full set of descriptors. Sequential forward selection shows how around 33\% of the descriptors do not add new information to the linear regression models, pointing towards redundancy in the descriptors.},
	booktitle = {{ICMPC}},
	author = {Gururani, Siddharth and Pati, Kumar Ashis and Wu, Chih-Wei and Lerch, Alexander},
	year = {2018},
	file = {Gururani et al. - 2018 - Analysis of Objective Descriptors for Music Perfor.pdf:H\:\\Docs\\zotero\\storage\\R6UEZBZK\\Gururani et al. - 2018 - Analysis of Objective Descriptors for Music Perfor.pdf:application/pdf}
}

@article{carter_using_2017,
	title = {Using {Artificial} {Intelligence} to {Augment} {Human} {Intelligence}},
	volume = {2},
	issn = {2476-0757},
	doi = {10.23915/distill.00009},
	abstract = {By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.},
	number = {12},
	journal = {Distill},
	author = {Carter, Shan and Nielsen, Michael},
	month = dec,
	year = {2017},
	pages = {e9},
	file = {Snapshot:H\:\\Docs\\zotero\\storage\\3XIIADVY\\aia.html:text/html}
}

@inproceedings{roberts_learning_2018,
	title = {Learning {Latent} {Representations} of {Music} to {Generate} {Interactive} {Musical} {Palettes}},
	booktitle = {Proceedings of the {Workshop} on {Intelligent} {Music} {Interfaces} for {Listening} and {Creation} ({MILC}@{IUI})},
	publisher = {ACM},
	author = {Roberts, Adam and Engel, Jesse and Oore, Sageev and Eck, Douglas},
	year = {2018},
	file = {Full Text PDF:H\:\\Docs\\zotero\\storage\\ZBJFU3H5\\Roberts et al. - 2018 - Learning Latent Representations of Music to Genera.pdf:application/pdf}
}

@inproceedings{sturm_music_2016,
	title = {Music {Transcription} {Modelling} and {Composition} {Using} {Deep} {Learning}},
	copyright = {info:eu-repo/semantics/closedAccess},
	abstract = {We apply deep learning methods, specifically long short-term
memory (LSTM) networks, to music transcription modelling and composition.
We build and train LSTM networks using approximately 23,000
music transcriptions expressed with a high-level vocabulary (ABC notation),
and use them to generate new transcriptions. Our practical aim
is to create music transcription models useful in particular contexts of
music composition. We present results from three perspectives: 1) at the
population level, comparing descriptive statistics of the set of training
transcriptions and generated transcriptions; 2) at the individual level,
examining how a generated transcription reflects the conventions of a
music practice in the training transcriptions (Celtic folk); 3) at the application
level, using the system for idea generation in music composition.
We make our datasets, software and sound examples open and available:
https://github.com/IraKorshunova/folk-rnn.},
	author = {Sturm, Bob and Santos, João Felipe and Ben-Tal, Oded and Korshunova, Iryna},
	year = {2016},
	file = {Snapshot:H\:\\Docs\\zotero\\storage\\PF3UED5S\\7223530.html:text/html}
}

@inproceedings{gururani_attention_2019,
	address = {Delft},
	title = {An {Attention} {Mechanism} for {Music} {Instrument} {Recognition}},
	abstract = {While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, suchasMedleyDB,havestrongper-frameinstrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or ‘attend to’) specific time segments in the audio relevant to each instrument label leading to interpretable results.},
	booktitle = {{ISMIR}},
	author = {Gururani, Siddharth and Sharma, Mohit and Lerch, Alexander},
	year = {2019},
	file = {Gururani et al. - 2019 - An Attention Mechanism for Music Instrument Recogn.pdf:H\:\\Docs\\zotero\\storage\\R2D272ZL\\Gururani et al. - 2019 - An Attention Mechanism for Music Instrument Recogn.pdf:application/pdf}
}

@inproceedings{pati_latent_2019,
	address = {Long Beach},
	title = {Latent {Space} {Regularization} for {Explicit} {Control} of {Musical} {Attributes}},
	abstract = {Deep generative models for music are often restrictive since they do not allow users any meaningful control over the generated music. To address this issue, we propose a novel latent space regularization technique which is capable of structuring the latent space of a deep generative model by encoding musically meaningful attributes along speciﬁc dimensions of the latent space. This, in turn, can provide users with explicit control over these attributes during inference and thereby, help design intuitive musical interfaces to enhance creative workﬂows.},
	booktitle = {{ICML} {Machine} {Learning} for {Music} {Discovery} {Workshop} ({ML}4MD), {Extended} {Abstract}},
	author = {Pati, Ashis and Lerch, Alexander},
	year = {2019},
	file = {Pati and Lerch - Latent Space Regularization for Explicit Control o.pdf:H\:\\Docs\\zotero\\storage\\V4QI5IQE\\Pati and Lerch - Latent Space Regularization for Explicit Control o.pdf:application/pdf}
}

@inproceedings{hung_multi-task_2020,
	address = {Montreal},
	title = {Multi-{Task} {Learning} for {Instrument} {Activation} {Aware} {Music} {Source} {Separation}},
	booktitle = {{ISMIR}},	
	author = {Hung, Yun-Ning and Lerch, Alexander},
	year = {2020}
}

@inproceedings{huang_score-informed_2020,
	address = {Montreal},
	title = {Score-informed {Networks} for {Music} {Performance} {Assessment}},
	booktitle = {{ISMIR}},
	author = {Huang, Jiawen and Hung, Yun-Ning and Pati, K Ashis and Gururani, Siddharth and Lerch, Alexander},
	year = {2020},
	file = {Huang et al. - 2020 - Score-informed Networks for Music Performance Asse.pdf:H\:\\Docs\\zotero\\storage\\4TR9QND5\\Huang et al. - 2020 - Score-informed Networks for Music Performance Asse.pdf:application/pdf}
}

@inproceedings{yang_remixing_2020,
	address = {Naples, Italy},
	title = {Remixing {Music} with {Visual} {Conditioning}},
	abstract = {We propose a visually conditioned music remixing system by incorporating deep visual and audio models. The method is based on a state of the art audio-visual source separation model which performs music instrument source separation with video information. We modified the model to work with user-selected images instead of videos as visual input during inference to enable separation of audio-only content. Furthermore, we propose a remixing engine that generalizes the task of source separation into music remixing. The proposed method is able to achieve improved audio quality compared to remixing performed by the separate-and-add method with a state-of-the-art audio-visual source separation model.},
	booktitle = {{IEEE} {ISM}},
	author = {Yang, Li-Chia and Lerch, Alexander},
	year = {2020}
}



@inproceedings{zhao_sound_2018,
	title = {The {Sound} of {Pixels}},
	abstract = {We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.},
	language = {en},
	booktitle = {{ECCV}},
	author = {Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio},
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Zhao et al. - 2018 - The Sound of Pixels.pdf:H\:\\Docs\\zotero\\storage\\GHYYQZU4\\Zhao et al. - 2018 - The Sound of Pixels.pdf:application/pdf}
}


@article{pati_attribute-based_2020,
	title = {Attribute-based {Regularization} for {Latent} {Spaces} of {Variational} {Auto}-{Encoders}},
	url = {https://arxiv.org/pdf/2004.05485},
	doi = {10.1007/s00521-020-05270-2},
	journal = {Neural Computing and Applications},
	author = {Pati, K Ashis and Lerch, Alexander},
	year = {2020},
	file = {Pati and Lerch - 2020 - Attribute-based Regularization for Latent Spaces o.pdf:H\:\\Docs\\zotero\\storage\\GTVGIUIH\\Pati and Lerch - 2020 - Attribute-based Regularization for Latent Spaces o.pdf:application/pdf},
}

@inproceedings{gururani_semi-supervised_2021,
	address = {online},
	title = {Semi-{Supervised} {Audio} {Classification} with {Partially} {Labeled} {Data}},
	url = {https://arxiv.org/abs/2111.12761},
	booktitle = {Proceedings of the {IEEE} {International} {Symposium} on {Multimedia} ({ISM})},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {Gururani, Siddharth and Lerch, Alexander},
	year = {2021},
	file = {Gururani and Lerch - 2021 - Semi-Supervised Audio Classification with Partiall.pdf:H\:\\Docs\\zotero\\storage\\SXP4VF28\\Gururani and Lerch - 2021 - Semi-Supervised Audio Classification with Partiall.pdf:application/pdf},
}

@inproceedings{seshadri_improving_2021,
	address = {Online},
	title = {Improving {Music} {Performance} {Assessment} with {Contrastive} {Learning}},
	url = {https://arxiv.org/abs/2108.01711},
	abstract = {Several automatic approaches for objective music performance assessment (MPA) have been proposed in the past, however, existing systems are not yet capable of reliably predicting ratings with the same accuracy as professional judges. This study investigates contrastive learning as a potential method to improve existing MPA systems. Contrastive learning is a widely used technique in representation learning to learn a structured latent space capable of separately clustering multiple classes. It has been shown to produce state of the art results for image-based classification problems. We introduce a weighted contrastive loss suitable for regression tasks applied to a convolutional neural network and show that contrastive loss results in performance gains in regression tasks for MPA. Our results show that contrastive-based methods are able to match and exceed SoTA performance for MPA regression tasks by creating better class clusters within the latent space of the neural networks.},
	language = {en},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Seshadri, Pavan and Lerch, Alexander},
	year = {2021},
	pages = {8},
	file = {Seshadri and Lerch - IMPROVING MUSIC PERFORMANCE ASSESSMENT WITH CONTRA.pdf:H\:\\Docs\\zotero\\storage\\GQWKT46M\\Seshadri and Lerch - IMPROVING MUSIC PERFORMANCE ASSESSMENT WITH CONTRA.pdf:application/pdf},
}

@inproceedings{pati_is_2021,
	address = {Online},
	title = {Is {Disentanglement} {Enough}? {On} {Latent} {Representations} for {Controllable} {Music} {Generation}},
	url = {https://arxiv.org/abs/2108.01450},
	abstract = {Improving controllability or the ability to manipulate one or more attributes of the generated data has become a topic of interest in the context of deep generative models of music. Recent attempts in this direction have relied on learning disentangled representations from data such that the underlying factors of variation are well separated. In this paper, we focus on the relationship between disentanglement and controllability by conducting a systematic study using different supervised disentanglement learning algorithms based on the Variational Auto-Encoder (VAE) architecture. Our experiments show that a high degree of disentanglement can be achieved by using different forms of supervision to train a strong discriminative encoder. However, in the absence of a strong generative decoder, disentanglement does not necessarily imply controllability. The structure of the latent space with respect to the VAE-decoder plays an important role in boosting the ability of a generative model to manipulate different attributes. To this end, we also propose methods and metrics to help evaluate the quality of a latent space with respect to the afforded degree of controllability.},
	language = {en},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Pati, Ashis and Lerch, Alexander},
	year = {2021},
	pages = {8},
	file = {Pati and Lerch - IS DISENTANGLEMENT ENOUGH ON LATENT REPRESENTATIO.pdf:H\:\\Docs\\zotero\\storage\\7FJCQQ9K\\Pati and Lerch - IS DISENTANGLEMENT ENOUGH ON LATENT REPRESENTATIO.pdf:application/pdf},
}

@inproceedings{watcharasupat_evaluation_2021,
	address = {Online},
	title = {Evaluation of {Latent} {Space} {Disentanglement} in the {Presence} of {Interdependent} {Attributes}},
	url = {http://arxiv.org/abs/2110.05587},
	abstract = {Controllable music generation with deep generative models has become increasingly reliant on disentanglement learning techniques. However, current disentanglement metrics, such as mutual information gap (MIG), are often inadequate and misleading when used for evaluating latent representations in the presence of interdependent semantic attributes often encountered in real-world music datasets. In this work, we propose a dependency-aware information metric as a drop-in replacement for MIG that accounts for the inherent relationship between semantic attributes.},
	urldate = {2021-11-11},
	booktitle = {Late {Breaking} {Demo} ({Extended} {Abstract}), {Proceedings} of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Watcharasupat, Karn N and Lerch, Alexander},
	year = {2021},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Information Retrieval, Computer Science - Information Theory},
	annote = {Comment: Submitted to the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference},
	file = {arXiv.org Snapshot:H\:\\Docs\\zotero\\storage\\5RS2Y85B\\2110.html:text/html;Watcharasupat and Lerch - 2021 - Evaluation of Latent Space Disentanglement in the .pdf:H\:\\Docs\\zotero\\storage\\KHFXB4JB\\Watcharasupat and Lerch - 2021 - Evaluation of Latent Space Disentanglement in the .pdf:application/pdf},
}

@article{watcharasupat_latte_2022,
	title = {Latte: {Cross}-framework {Python} {Package} for {Evaluation} of {Latent}-based {Generative} {Models}},
	issn = {26659638},
	shorttitle = {Latte},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2665963822000033},
	doi = {10.1016/j.simpa.2022.100222},
	abstract = {Latte (for LATent Tensor Evaluation) is a Python library for evaluation of latent-based generative models in the fields of disentanglement learning and controllable generation. Latte is compatible with both PyTorch and TensorFlow/Keras, and provides both functional and modular APIs that can be easily extended to support other deep learning frameworks. Using NumPy-based and framework-agnostic implementation, Latte ensures reproducible, consistent, and deterministic metric calculations regardless of the deep learning framework of choice.},
	language = {en},
	urldate = {2022-01-13},
	journal = {Software Impacts},
	author = {Watcharasupat, Karn N and Lee, Junyoung and Lerch, Alexander},
	year = {2022},
	pages = {100222},
	file = {Watcharasupat et al. - 2022 - Latte Cross-framework Python package for evaluati.pdf:H\:\\Docs\\zotero\\storage\\4TTG3MIX\\Watcharasupat et al. - 2022 - Latte Cross-framework Python package for evaluati.pdf:application/pdf},
}

@inproceedings{hung_feature-informed_2022,
	address = {Belgrade, Serbia},
	title = {Feature-informed {Embedding} {Space} {Regularization} for {Audio} {Classification}},
	url = {http://arxiv.org/abs/2206.04850},
	doi = {10.48550/arXiv.2206.04850},
	abstract = {Feature representations derived from models pre-trained on large-scale datasets have shown their generalizability on a variety of audio analysis tasks. Despite this generalizability, however, task-specific features can outperform if sufficient training data is available, as specific task-relevant properties can be learned. Furthermore, the complex pre-trained models bring considerable computational burdens during inference. We propose to leverage both detailed task-specific features from spectrogram input and generic pre-trained features by introducing two regularization methods that integrate the information of both feature classes. The workload is kept low during inference as the pre-trained features are only necessary for training. In experiments with the pre-trained features VGGish, OpenL3, and a combination of both, we show that the proposed methods not only outperform baseline methods, but also can improve state-of-the-art models on several audio classification tasks. The results also suggest that using the mixture of features performs better than using individual features.},
	booktitle = {Proceedings of the {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Hung, Yun-Ning and Lerch, Alexander},
	year = {2022},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:H\:\\Docs\\zotero\\storage\\RGA94GDW\\Hung and Lerch - 2022 - Feature-informed Embedding Space Regularization Fo.pdf:application/pdf;arXiv.org Snapshot:H\:\\Docs\\zotero\\storage\\T7VT6VCV\\2206.html:text/html},
}

@inproceedings{ma_representation_2022,
	address = {Bangalore, IN},
	title = {Representation {Learning} for the {Automatic} {Indexing} of {Sound} {Effects} {Libraries}},
	url = {http://arxiv.org/abs/2208.09096},
	doi = {10.48550/arXiv.2208.09096},
	abstract = {Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness.},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Ma, Alison B and Lerch, Alexander},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09096 [cs, eess]},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted at the 23rd International Society for Music Information Retrieval Conference (ISMIR 2022), 10 pages, 7 figures},
	file = {arXiv.org Snapshot:H\:\\Docs\\zotero\\storage\\LKE7ZHG2\\2208.html:text/html;Ma_Lerch_2022_Representation Learning for the Automatic Indexing of Sound Effects Libraries.pdf:H\:\\Docs\\zotero\\storage\\BGVIGZ8E\\Ma_Lerch_2022_Representation Learning for the Automatic Indexing of Sound Effects Libraries.pdf:application/pdf},
}

@inproceedings{vinay_evaluating_2022,
	address = {Bangalore, IN},
	title = {Evaluating {Generative} {Audio} {Systems} and their {Metrics}},
	url = {http://arxiv.org/abs/2209.00130},
	doi = {10.48550/arXiv.2209.00130},
	abstract = {Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems.},
	urldate = {2022-09-03},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Vinay, Ashvala and Lerch, Alexander},
	month = aug,
	year = {2022},
	note = {arXiv:2209.00130 [cs, eess]},
	keywords = {Computer Science - Sound, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted at ISMIR 2022},
	file = {arXiv.org Snapshot:H\:\\Docs\\zotero\\storage\\BIZEHSRP\\2209.html:text/html;Vinay_Lerch_2022_Evaluating generative audio systems and their metrics.pdf:H\:\\Docs\\zotero\\storage\\UDA8AL4V\\Vinay_Lerch_2022_Evaluating generative audio systems and their metrics.pdf:application/pdf},
}

@inproceedings{chen_music_2023,
	address = {Bergen, Norway},
	title = {Music {Instrument} {Classification} {Reprogrammed}},
	url = {https://arxiv.org/abs/2211.08379},
	booktitle = {Proceedings of the {International} {Conference} on {Multimedia} {Modeling} ({MMM})},
	author = {Chen, Hsin-Hung and Lerch, Alexander},
	year = {2023},
}

@inproceedings{gong_ast_2021,
	address = {Brno, Czechia},
	title = {{AST}: {Audio} {Spectrogram} {Transformer}},
	shorttitle = {{AST}},
	url = {http://arxiv.org/abs/2104.01778},
	abstract = {In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6\% accuracy on ESC-50, and 98.1\% accuracy on Speech Commands V2.},
	urldate = {2022-04-17},
	booktitle = {Proceedings of {Interspeech}},
	author = {Gong, Yuan and Chung, Yu-An and Glass, James},
	month = jul,
	year = {2021},
	note = {arXiv: 2104.01778},
	keywords = {Computer Science - Sound, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\coduser\\Zotero\\storage\\IWET6PMQ\\Gong et al. - 2021 - AST Audio Spectrogram Transformer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\coduser\\Zotero\\storage\\AFC5FI5Z\\2104.html:text/html},
}

@inproceedings{bachman_learning_2014,
	title = {Learning with {Pseudo}-{Ensembles}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/hash/66be31e4c40d676991f2405aaecc6934-Abstract.html},
	abstract = {We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.},
	urldate = {2021-12-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bachman, Philip and Alsharif, Ouais and Precup, Doina},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\coduser\\Zotero\\storage\\EAWJWDYW\\Bachman et al. - 2014 - Learning with Pseudo-Ensembles.pdf:application/pdf;Full Text PDF:C\:\\Users\\coduser\\Zotero\\storage\\7L3CGU45\\Bachman et al. - 2014 - Learning with Pseudo-Ensembles.pdf:application/pdf},
}

@article{fonseca_addressing_2020,
	title = {Addressing {Missing} {Labels} in {Large}-{Scale} {Sound} {Event} {Recognition} {Using} a {Teacher}-{Student} {Framework} {With} {Loss} {Masking}},
	volume = {27},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2005.00878},
	doi = {10.1109/LSP.2020.3006378},
	abstract = {The study of label noise in sound event recognition has recently gained attention with the advent of larger and noisier datasets. This work addresses the problem of missing labels, one of the big weaknesses of large audio datasets, and one of the most conspicuous issues for AudioSet. We propose a simple and model-agnostic method based on a teacher-student framework with loss masking to first identify the most critical missing label candidates, and then ignore their contribution during the learning process. We find that a simple optimisation of the training label set improves recognition performance without additional computation. We discover that most of the improvement comes from ignoring a critical tiny portion of the missing labels. We also show that the damage done by missing labels is larger as the training set gets smaller, yet it can still be observed even when training with massive amounts of audio. We believe these insights can generalize to other large-scale datasets.},
	urldate = {2022-04-17},
	journal = {IEEE Signal Processing Letters},
	author = {Fonseca, Eduardo and Hershey, Shawn and Plakal, Manoj and Ellis, Daniel P. W. and Jansen, Aren and Moore, R. Channing and Serra, Xavier},
	year = {2020},
	note = {arXiv: 2005.00878},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1235--1239},
	file = {arXiv Fulltext PDF:C\:\\Users\\coduser\\Zotero\\storage\\NBITDBDG\\Fonseca et al. - 2020 - Addressing Missing Labels in Large-Scale Sound Eve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\coduser\\Zotero\\storage\\B42F29KS\\2005.html:text/html},
}

@inproceedings{ding_audio_2023,
	address = {Milan, Italy},
	title = {Audio {Embeddings} as {Teachers} for {Music} {Classification}},
	url = {http://arxiv.org/abs/2306.17424},
	doi = {10.48550/arXiv.2306.17424},
	abstract = {Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the increasing model complexity makes both training and inference computationally expensive. In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks. By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students. We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging. Results show that our method significantly improves the results in comparison to the identical model trained without the teacher's knowledge. This technique can also be combined with classical knowledge distillation approaches to further improve the model's performance.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Ding, Yiwei and Lerch, Alexander},
	year = {2023},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\coduser\\Zotero\\storage\\X6AZAGFY\\Ding and Lerch - 2023 - Audio Embeddings as Teachers for Music Classificat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\coduser\\Zotero\\storage\\L8SBVILX\\2306.html:text/html},
}

@inproceedings{qin_tuning_2019,
	address = {Brighton, UK},
	title = {Tuning {Frequency} {Dependency} in {Music} {Classification}},
	url = {http://www.musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/04/Qin-and-Lerch-2019-Tuning-Frequency-Dependency-in-Music-Classificatio.pdf},
	doi = {10.1109/ICASSP.2019.8683340},
	abstract = {Deep architectures have become ubiquitous in Music Information Retrieval (MIR) tasks, however, concurrent studies still lack a deep understanding of the input properties being evaluated by the networks. In this study, we show by the example of a Music Genre Classification system the potential dependency on the tuning frequency, an irrelevant and confounding variable. We generate adversarial samples through pitch-shifting the audio data and investigate the classification accuracy of the output depending on the pitch shift. We find the accuracy to be periodic with a period of one semitone, indicating that the system is utilizing tuning information. We show that proper data augmentation including pitch-shifts smaller than one semitone helps minimizing this problem and point out the need for carefully designed augmentation procedures in related MIR tasks.},
	language = {en},
	booktitle = {Proceedings of the {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {Qin, Yi and Lerch, Alexander},
	year = {2019},
	keywords = {music genre classification, convolutional recurrent neural networks, model evaluation, tuning frequency},
	pages = {401--405},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\coduser\\Zotero\\storage\\Y6WBISCR\\8683340.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\coduser\\Zotero\\storage\\5N3B8YV8\\Qin and Lerch - 2019 - Tuning Frequency Dependency in Music Classificatio.pdf:application/pdf;Qin and Lerch - 2019 - Tuning Frequency Dependency in Music Classificatio.pdf:C\:\\Users\\coduser\\Zotero\\storage\\DMSYJWCV\\Qin and Lerch - 2019 - Tuning Frequency Dependency in Music Classificatio.pdf:application/pdf},
}
